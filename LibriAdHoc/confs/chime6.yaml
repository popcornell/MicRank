training:
  batch_size: 1
  batch_size_val: 1
  num_workers: 12 # change according to your cpu
  n_epochs: 1 # max num epochs
  early_stop_patience: 10 # Same as number of epochs by default, so no early stopping used
  accumulate_batches: 1
  gradient_clip: 0 # 0 no gradient clipping
  backend:
  validation_interval: 1 # perform validation every X epoch, 1 default
  seed: 42
  delta_wer: 0.01
  discard_shorter: 32000
  sorted: random
  rankscore: nwer
  ranking: listwise
  loss: xentropy
scheduler:
  factor: 0.5
  patience: 5
scaler:
  statistic: instance # instance or dataset-wide statistic
  normtype:   # minmax or standard or mean normalization
  dims: [0, 1, 2, 3] # dimensions over which normalization is applied
  savepath: # path to scaler checkpoint
data: # change with your paths if different.
  train_json: ./parsed_chime6/train.json
  dev_json: ./parsed_chime6/dev.json
  test_json: ./parsed_chime6/eval.json
  fs: 16000
opt:
  lr: 0.00015
  betas: [0.9, 0.999]
  #weight_decay: !!float 1e-7
feats_type: waveform
fbank:
  n_mels: 64
  power: 1
  sample_rate: 16000
  n_fft: 400
  hop_length: 200
tcn:
  in_chan: 64
  dropout: 0.1
  bn_chan: 64
  hid_chan: 128
crnn:
  dropout: 0.5
  rnn_layers: 2
  n_in_channel: 1
  nclass: 1
  attention: True
  n_RNN_cell: 128
  activation: glu
  rnn_type: BGRU
  kernel_size: [ 3, 3, 3, 3, 3, 3, 3 ]
  padding: [ 1, 1, 1, 1, 1, 1, 1 ]
  stride: [ 1, 1, 1, 1, 1, 1, 1 ]
  nb_filters: [ 16, 32, 64, 128, 128, 128, 128 ]
  pooling: [ [ 2, 2 ], [ 2, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ] ]
  dropout_recurrent: 0
  normalization: layer
augmentation:
  rolling: True
  specaugm:
    freqs:
    time: