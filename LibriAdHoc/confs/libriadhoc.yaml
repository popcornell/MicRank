training:
  batch_size: 1
  batch_size_val: 1
  num_workers: 12 # change according to your cpu
  n_epochs: 100 # max num epochs
  early_stop_patience: 10 # Same as number of epochs by default, so no early stopping used
  accumulate_batches: 1
  gradient_clip: 5 # 0 no gradient clipping
  backend:
  validation_interval: 1 # perform validation every X epoch, 1 default
  seed: 42
  delta_wer: 0.01
  discard_shorter: 8000
  chunk: False
  sorted: random
  rankscore: nwer
  ranking: listwise
  loss: listnet
  save_selected: False
scheduler:
  factor: 0.5
  patience: 5
scaler:
  statistic: instance # instance or dataset-wide statistic
  normtype:  minmax #or standard or mean normalization
  dims: [1, 2] # dimensions over which normalization is applied
  savepath: # path to scaler checkpoint
data: # change with your paths if different.
  train_json: ./parsed/train.json
  dev_json: ./parsed/dev.json
  test_json: ./parsed/test.json
  fs: 16000
opt:
  lr: !!float 1e-4
  #betas: [0.9, 0.999]
  #momentum: 0.9
  #weight_decay: !!float 1e-7
feats_type: fbank
fbank:
  n_mels: 40
  power: 2
  sample_rate: 16000
  n_fft: 400
  hop_length: 200
tcn:
  in_chan: 40
  dropout: 0.0
  bn_chan: 64
  hid_chan: 128
  n_blocks: 5
crnn:
  dropout: 0.5
  rnn_layers: 2
  n_in_channel: 1
  nclass: 1
  attention: True
  n_RNN_cell: 128
  activation: glu
  rnn_type: BGRU
  kernel_size: [ 3, 3, 3, 3, 3, 3, 3 ]
  padding: [ 1, 1, 1, 1, 1, 1, 1 ]
  stride: [ 1, 1, 1, 1, 1, 1, 1 ]
  nb_filters: [ 16, 32, 64, 128, 128, 128, 128 ]
  pooling: [ [ 2, 2 ], [ 2, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ] ]
  dropout_recurrent: 0
  normalization: layer
augmentation:
  speed: True
  shift: True
  specaugm:
    freqs: 4
    time: 4

